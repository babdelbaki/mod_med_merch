{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import date\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_channel(link):\n",
    "    # extracts number of videos, average view count, and median view count for past 10 videos\n",
    "    try:\n",
    "        html = urllib.urlopen(link + \"/videos\")\n",
    "        soup2 = BeautifulSoup(html.read())\n",
    "    except:\n",
    "        return (\"ERR0\", \"ERR0\", \"ERR0\")\n",
    "    \n",
    "    count = 0\n",
    "    views_list = []\n",
    "    \n",
    "    #print(soup2)\n",
    "    try:\n",
    "        scripts = soup2.find_all('script')\n",
    "        #for script in scripts:\n",
    "            # print(\"\\n\\n\", script)\n",
    "        vids = json.loads(scripts[27].get_text()[31:-110])\n",
    "        # print(vids)\n",
    "        vid_list_1 = vids[\"contents\"][\"twoColumnBrowseResultsRenderer\"][\"tabs\"][1][\"tabRenderer\"]\n",
    "        vid_list_2 = vid_list_1[\"content\"][\"sectionListRenderer\"][\"contents\"][0][\"itemSectionRenderer\"][\"contents\"]\n",
    "        video_list = vid_list_2[0][\"gridRenderer\"][\"items\"]\n",
    "    except:\n",
    "        return (\"ERR1\", \"ERR1\", \"ERR1\")\n",
    "    \n",
    "    try:\n",
    "        for video in video_list:\n",
    "            vid_info = video[\"gridVideoRenderer\"]\n",
    "            upload_time = vid_info[\"publishedTimeText\"][\"simpleText\"]\n",
    "\n",
    "            # check upload date - stop loop once 10 videos have been reached\n",
    "            if count == 10: break\n",
    "\n",
    "            # count number of videos in last month\n",
    "            count += 1\n",
    "\n",
    "\n",
    "            # get view count for each video\n",
    "            # extract all characters except for \" views\" and convert to integer\n",
    "            views = int(vid_info[\"viewCountText\"][\"simpleText\"][:-6].replace(\",\", \"\"))\n",
    "\n",
    "            views_list.append(views)\n",
    "    except:\n",
    "        return (\"ERR2\", \"ERR2\", \"ERR2\")\n",
    "    \n",
    "    # error handling if no videos are found in past month\n",
    "    try:\n",
    "        mean_views = statistics.mean(views_list)\n",
    "        median_views = statistics.median(views_list)\n",
    "    except:\n",
    "        return (\"ERR3\", \"ERR3\", \"ERR3\")\n",
    "    \n",
    "    # print(link, \"\\t\", count, mean_views, median_views)\n",
    "    \n",
    "    return (count, mean_views, median_views)\n",
    "\n",
    "def round_5000(views):\n",
    "    try:\n",
    "        ans = int(round(float(views) / 5000) * 5)\n",
    "    except:\n",
    "        ans = \"ERR\"\n",
    "    return ans\n",
    "def calculate_money(views):\n",
    "    try:\n",
    "        ans = int(float(views) * 5 * 6.2)\n",
    "    except:\n",
    "        ans = \"ERR\"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 79461.4, 23439.5)\n"
     ]
    }
   ],
   "source": [
    "print(check_channel(\"https://www.youtube.com/c/BleepinjeepPage\"))\n",
    "#print(check_channel(\"http://www.youtube.com/channel/UCPGhzlxltApd7GEngwgdPig\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Name\n",
      "0              John Skinner\n",
      "1      Japan Food Adventure\n",
      "2               Alliesrasza\n",
      "3                NoobHunter\n",
      "4                  amandabb\n",
      "...                     ...\n",
      "13842            gamerZsoul\n",
      "13843           FÁBIO GAMER\n",
      "13844            tombino546\n",
      "13845         NtenseFit Way\n",
      "13846      BOSSO PRODUCTION\n",
      "\n",
      "[13847 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "contacts = pd.read_csv(\"/Users/basse/OneDrive/Documents/Modern Media Merch/contact names.csv\")\n",
    "print(contacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"John Skinner\" in contacts.values\n",
    "# date.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adapt code to use Scrapy - potentially much faster and feasible to run on cloud\n",
    "# Run current code on pythonanywhere and see how much cpu time it takes for 10 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Ø²Ù…Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…\n",
      "2\n",
      "3\n",
      "Virna Huerta\n",
      "4\n",
      "5\n",
      "kim soc Camera\n",
      "6\n",
      "7\n",
      "Alpha Venom\n",
      "8\n",
      "9\n",
      "Pablo and Co\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Rina in London\n",
      "16\n",
      "Sen\n",
      "17\n",
      "SOKHA RFA\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "Sexy dance official\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "djofer officiel\n",
      "38\n",
      "SKREEK\n",
      "39\n",
      "Khadija Reeves\n",
      "40\n",
      "41\n",
      "Kresnt\n",
      "42\n",
      "43\n",
      "SmileyMe\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "SidLyrics SL\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [Name, Subscribers, Genre, Link, Num videos, Median views, Projected Earnings, Mean views, Date Added]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# https://www.channelcrawler.com/eng\n",
    "\n",
    "# initialize dataframe\n",
    "df = pd.DataFrame(columns = [\"Name\", \"Subscribers\", \"Genre\", \"Link\", \"Num videos\", \n",
    "                            \"Median views\", \"Projected Earnings\", \"Mean views\", \"Date Added\"])\n",
    "\n",
    "# id for search\n",
    "number = 4062 # all genres 1k + \n",
    "# number = 1553 # all genres 10k+\n",
    "# number = 255613 all genres 25k - 3 mil\n",
    "# number = 197785 all genres 100k - 3 mil\n",
    "# number = 195112 gaming 100k - 3 mil\n",
    "# number = 132065 gaming 100k - 2mil\n",
    "# number = 111710  gaming old\n",
    "# number = 136782 all genres\n",
    "search_link = \"https://www.channelcrawler.com/eng/results/\" + str(number)\n",
    "\n",
    "# get current list of contacts\n",
    "contacts = pd.read_csv(\"/Users/basse/OneDrive/Documents/Modern Media Merch/contact names.csv\")\n",
    "for page in range(1, 51):\n",
    "    # loop through each page of search results\n",
    "    print(page)\n",
    "    html = urllib.urlopen(search_link + \"/page:\" + str(page))\n",
    "    soup = BeautifulSoup(html.read())\n",
    "\n",
    "    # print(soup.body)\n",
    "\n",
    "    # extract channel items from webpage\n",
    "    channels = soup.find_all(\"div\", class_=\"channel col-xs-12 col-sm-4 col-lg-3\")\n",
    "    # print(channels)\n",
    "\n",
    "    # for each channel\n",
    "    # get channel name, link, genre, and subscribers from channel crawler\n",
    "    # visit youtube page and extract number of videos, average views per video, and max views in the last month\n",
    "    for channel in channels:\n",
    "        try:\n",
    "            name = channel.find(\"a\").get(\"title\")\n",
    "\n",
    "            # check if channel name is already in contact list, skip if so\n",
    "            if name in contacts.values: continue\n",
    "\n",
    "            print(name)\n",
    "\n",
    "            link = channel.find(\"a\").get(\"href\")\n",
    "            genre = channel.find_all(\"small\")[0].get_text().strip()\n",
    "            subscribers = channel.find_all(\"small\")[1].get_text().strip()[:4]\n",
    "\n",
    "\n",
    "            print(\"\\t\", link)\n",
    "\n",
    "\n",
    "            num_vids, mean_views, median_views = check_channel(link)\n",
    "            \n",
    "            # round to nearest 5000\n",
    "            mean_views = round_5000(mean_views)\n",
    "            median_views = round_5000(median_views)\n",
    "            projected_earns = calculate_money(median_views)\n",
    "\n",
    "            print(\"\\t\", median_views, \"k\")\n",
    "\n",
    "            df = df.append({\"Name\": name, \"Link\": link + \"/about\", \"Genre\": genre,\n",
    "                            \"Subscribers\": subscribers, \"Num videos\": num_vids,\n",
    "                           \"Mean views\": mean_views, \"Median views\": median_views, \n",
    "                            \"Projected Earnings\" : projected_earns,\n",
    "                           \"Date Added\": date.today().strftime(\"%Y-%m-%d\")},\n",
    "                           ignore_index = True)\n",
    "\n",
    "        except:\n",
    "            # if fails for any reason, move on to next channel\n",
    "            continue\n",
    "\n",
    "        # print(df)\n",
    "df.to_csv(\"/Users/basse/OneDrive/Documents/Modern Media Merch/new contact data v2.csv\", mode = \"a\", \n",
    "      index = False, header = False)\n",
    "\n",
    "contact_list = contacts.merge(df.loc[:, \"Name\"], \"outer\")\n",
    "contact_list.to_csv(\"/Users/basse/OneDrive/Documents/Modern Media Merch/contact names.csv\", index = False)\n",
    "\n",
    "    \n",
    "print(len(df.index))   \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Revenue\"] = df[\"Average views last month\"] * 0.005 * 4.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contact_list = pd.concat(df[\"Name\"], contacts.values)\n",
    "# df.loc[:, \"Name\"]\n",
    "df.to_csv(\"/Users/Able Care/Documents/Modern Media Merch/new contact data.csv\", mode = \"a\", header = False)\n",
    "\n",
    "contact_list = contacts.merge(df.loc[:, \"Name\"], \"outer\")\n",
    "contact_list.to_csv(\"/Users/Able Care/Documents/Modern Media Merch/contact names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. More on Web Scraping\n",
    "* **BeautifulSoup** Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* **Selenium:** https://selenium-python.readthedocs.io/index.html\n",
    "    * Web automation and scraping; dynamic GET and POST requests; can interact with dynamic web pages, forms, etc.\n",
    "* **Scrapy:** https://scrapy.org/\n",
    "    * Optimized web crawling tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\basse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\basse\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# one-off update code\n",
    "contact_list = pd.read_clipboard()\n",
    "\n",
    "contact_list[\"Channel Link\"] = contact_list[\"Channel Link\"].str.replace(\"/about\", \"\")\n",
    "contact_list_exp = contact_list\n",
    "\n",
    "def get_views(link):\n",
    "    return check_channel(link)\n",
    "\n",
    "contact_list_exp[\"Vid Count\"] = pd.Series()\n",
    "contact_list_exp[\"Mean Views\"] = pd.Series()\n",
    "contact_list_exp[\"Median Views\"] = pd.Series()\n",
    "\n",
    "contact_list_exp[\"Vid Count\"], contact_list_exp[\"Mean Views\"], contact_list_exp[\"Median Views\"] = zip(*contact_list_exp[\"Channel Link\"].map(get_views))\n",
    "\n",
    "# round views, calculate projected earnings, add about to end of link\n",
    "contact_list_exp[\"Mean Views\"] = contact_list_exp[\"Mean Views\"].apply(round_5000)\n",
    "contact_list_exp[\"Median Views\"] = contact_list_exp[\"Median Views\"].apply(round_5000)\n",
    "contact_list_exp[\"Projected Money\"] = contact_list_exp[\"Median Views\"].apply(calculate_money)\n",
    "contact_list_exp[\"Channel Link\"] = contact_list_exp[\"Channel Link\"] + \"/about\"\n",
    "contact_list_exp.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# migration to scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import statistics\n",
    "\n",
    "from twisted.internet import reactor\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelCrawlerSpider(scrapy.Spider):\n",
    "    name = \"channel\"\n",
    "    \n",
    "    number = 4062\n",
    "    base_url = \"https://www.channelcrawler.com/eng/results/\" + str(number) + \"/page:\"\n",
    "    \n",
    "    channel_crawler_links = []\n",
    "    for i in range(1, 3):\n",
    "        channel_crawler_links.append(base_url + str(i))\n",
    "    \n",
    "    start_urls = channel_crawler_links\n",
    "    custom_settings = { 'DOWNLOAD_DELAY': 1 }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # extract channel links from page\n",
    "        links = response.xpath('//*[@id=\"main-content\"]//div//div//h4//a/@href').getall()     \n",
    "        links = [link + \"/videos\" for link in links]\n",
    "        '''\n",
    "        channel_names = response.xpath('//*[@id=\"main-content\"]//div//div//h4//a/text()').getall()    \n",
    "        genres = response.xpath('//*[@id=\"main-content\"]//div//div//small//b/text()').getall()\n",
    "        \n",
    "        # get rid of example videos which can also be picked up\n",
    "        text = \"Example\"\n",
    "        genres_cleaned = [x.strip() for x in genres if text not in x]\n",
    "        \n",
    "        subcount = response.xpath('//*[@id=\"main-content\"]//div//div//p//small/text()').getall()\n",
    "        # restrict strings to those containing subscriber, then extract text before the space (subscriber count)\n",
    "        text = \"Subscribers\"\n",
    "        subcount_cleaned = [x.strip().split(\" \")[0] for x in subcount if text in x]\n",
    "        '''\n",
    "        yield from response.follow_all(links, self.parse_channel)\n",
    "        \n",
    "    def parse_channel(self, response):\n",
    "        # extract view counts from youtube videos \n",
    "        view_data = response.xpath(\"//script[contains(., 'viewCount')]/text()\").extract()[0]\n",
    "        view_list = re.findall('shortViewCountText\":{\"simpleText\":\"(.+?) view', view_data)\n",
    "        view_list_cleaned = []\n",
    "        \n",
    "        # drop unnecessary items from view list\n",
    "        for view in view_list:\n",
    "            if len(view) < 7: \n",
    "                # account for k vs m\n",
    "                if view[-1] == \"K\":\n",
    "                    view_list_cleaned.append(float(view[0:-1]) * 1000)\n",
    "                elif view[-1] == \"M\":\n",
    "                    view_list_cleaned.append(float(view[0:-1]) * 1000000)\n",
    "                else:\n",
    "                    view_list_cleaned.append(float(view))\n",
    "            \n",
    "            if len(view_list_cleaned) == 10: break\n",
    "        \n",
    "        median_views = statistics.median(view_list_cleaned)\n",
    "        num_videos = len(view_list_cleaned)\n",
    "        \n",
    "        # get channel name, subscriber count\n",
    "        channel_data = str(response.body)\n",
    "        channel_name = re.findall('<meta name=\"title\" content=\"(.+?)\">', channel_data)[0]\n",
    "        sub_count = re.findall('\"subscriberCountText\":{\"simpleText\":\"(.+?) subscribers', channel_data)[0]\n",
    "        \n",
    "        print(channel_name, sub_count, response.url, median_views, num_videos, sep = \"\\t\")\n",
    "        \n",
    "        yield{\n",
    "            \"Channel Name\" : channel_name,\n",
    "            \"Channel link\" : response.url,\n",
    "            \"Subscribers\": sub_count,\n",
    "            \"Median Views\": median_views,\n",
    "            \"Number of videos\": num_videos\n",
    "        }\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basse\\Anaconda3\\lib\\site-packages\\scrapy\\extensions\\feedexport.py:239: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Lambert\t5.04K\thttps://www.youtube.com/channel/UCjWIxEQuYgWKq0GHMW2IbOw/videos\t62.0\t10\n",
      "Samsul Bhe\t15.7K\thttps://www.youtube.com/channel/UCIEouqUGMEANz6WBRiP3Kfg/videos\t57.0\t10\n",
      "Josh T\t2.22K\thttps://www.youtube.com/channel/UChQeacYZdpxjQbh9EsO9r-w/videos\t521.5\t10\n",
      "powerfulukltd\t73.3K\thttps://www.youtube.com/channel/UCjCbH52t-y0TPd6Y4CElvFw/videos\t3950.0\t10\n",
      "RBLstylelife\t28.2K\thttps://www.youtube.com/channel/UCBFgGNi7zmRUPy9Iv8S6Flw/videos\t708.0\t10\n",
      "PALE1080\t33.9K\thttps://www.youtube.com/channel/UCzDXQ7i0mR5_PvT56YpPOXA/videos\t6150.0\t10\n",
      "ANGEL PEQUE\\xc3\\x91O\t15.5K\thttps://www.youtube.com/channel/UCNS6_G-SajjJYq7Nb98m82w/videos\t349.0\t10\n",
      "Tonny L Nielsen\t3.11K\thttps://www.youtube.com/channel/UCDzfexBSQCc0zssZPxgN83A/videos\t103.5\t10\n",
      "PD\t6.9K\thttps://www.youtube.com/channel/UC4O9wfeYvwwneoelzEiwVqA/videos\t598.0\t10\n",
      "CBS Denver\t47.5K\thttps://www.youtube.com/channel/UC_-gT7OYiRCK9Sp8SIv9WgQ/videos\t68.0\t10\n",
      "MrcreeperGam1ng\t2.77K\thttps://www.youtube.com/channel/UCVqL7pZqLUwU6l6SJUVqImQ/videos\t298.5\t10\n",
      "Travolt Ainda Joga\t1.32K\thttps://www.youtube.com/channel/UCa3zlmw1V2uMFn8cFzZCQVQ/videos\t124.5\t10\n",
      "GuineaDad\t41.7K\thttps://www.youtube.com/channel/UCBnD6FWitIakVK8ID-QGT1A/videos\t4250.0\t10\n",
      "CheekyMonkeyGaming\t2.51K\thttps://www.youtube.com/channel/UCJbVvWDir8fdZ0-ewHFUAJA/videos\t136.0\t10\n",
      "KidaniStars\t359K\thttps://www.youtube.com/channel/UCpg9aDs4Brj4atWcoHYPtFQ/videos\t317.5\t10\n",
      "CoinCas Tuski Guy 433\t1.09K\thttps://www.youtube.com/channel/UCpg29VceJFgKXh2I_CmQhjA/videos\t149.5\t10\n",
      "carbikenetwork\t23.8K\thttps://www.youtube.com/channel/UCj0WC5IO5KyxihrO0w9ACWA/videos\t215.0\t10\n",
      "CKJ Crypto News\t24.1K\thttps://www.youtube.com/channel/UCmexsZ6pFvmXa9hOnnyRz5A/videos\t2000.0\t10\n",
      "News24Info\t276K\thttps://www.youtube.com/channel/UCxaJP5WrDPfR9yG7Xoyx0xQ/videos\t4050.0\t10\n",
      "Mirchi Bollywood\t801K\thttps://www.youtube.com/channel/UCLGXDicZMgUSMuyYO7_t7vQ/videos\t406.5\t10\n",
      "CNA\t1.04M\thttps://www.youtube.com/channel/UC83jt4dlz1Gjl58fzQrrKZg/videos\t100.0\t10\n",
      "Travis Heinze\t22.9K\thttps://www.youtube.com/channel/UCy2p0ys6SY4j7Q4OIL1xk_A/videos\t1950.0\t10\n",
      "Shyam Sadhu\t287K\thttps://www.youtube.com/channel/UC3XAT8oBjL2RaqfblESHW_A/videos\t598.0\t10\n",
      "Indian Youtuber Pratibha\t4.99K\thttps://www.youtube.com/channel/UCrKgZe4CQa81SYsWXmXHU7g/videos\t77.0\t10\n",
      "The Epoch Times\t281K\thttps://www.youtube.com/channel/UCeLc4heXqG9dtL7jiPHMBfQ/videos\t6200.0\t10\n",
      "MotorCarTube\t47.9K\thttps://www.youtube.com/channel/UCDItx0ktzWqsZgOVaBpPqpw/videos\t49.5\t10\n",
      "#Bestestimes\t2.3K\thttps://www.youtube.com/channel/UCTglpLz3TQnCfqpxsnZO1cA/videos\t19.0\t10\n",
      "Sajam\t70.8K\thttps://www.youtube.com/channel/UCVsmYrE8-v3VS7XWg3cXp9g/videos\t5450.0\t10\n",
      "The Rideshare Hub\t43.8K\thttps://www.youtube.com/channel/UC-ivJVv3GtB_w_lpk_WQxHw/videos\t321.0\t10\n",
      "Rumiin\t9K\thttps://www.youtube.com/channel/UCk3H3Z0JPi2gIastv5_dMZg/videos\t160.5\t10\n",
      "Too LIT Mafia\t382K\thttps://www.youtube.com/channel/UCyUddxXedBM1h6Qc5weaxpQ/videos\t2200.0\t10\n",
      "Cplus Tv\t780K\thttps://www.youtube.com/channel/UC3aNZXndc9tRpYKYKSYUbrg/videos\t1950.0\t10\n",
      "WCIA News\t4.3K\thttps://www.youtube.com/channel/UC4aWK7nhEyAkpP5-8YRdvJA/videos\t6.0\t10\n",
      "Wayne Kibler&#39;s RC Adventures\t1.24K\thttps://www.youtube.com/channel/UCygCQD8Hwz3AwPVClhcEqlw/videos\t100.5\t10\n",
      "CNBC Television\t886K\thttps://www.youtube.com/channel/UCrp_UI8XtuYfpiqluWLD7Lw/videos\t269.0\t10\n",
      "Dogmal\t76.5K\thttps://www.youtube.com/channel/UCp5HYqcl8gpbC7cyRW-DV1Q/videos\t1500.0\t10\n",
      "MOMables - Laura Fuentes\t87.6K\thttps://www.youtube.com/channel/UC0oO2ekgH83Zc3j7SOvT_2Q/videos\t1400.0\t10\n",
      "Pigeons Mania\t5.89K\thttps://www.youtube.com/channel/UCZ91NwZB7o3S-O-W0F53mpg/videos\t68.0\t10\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    runner = CrawlerRunner({\n",
    "        'FEED_URI': 'youtuberdata.csv',\n",
    "        'FEED_FORMAT': 'csv',\n",
    "    })\n",
    "    \n",
    "    d = runner.crawl(ChannelCrawlerSpider)\n",
    "    d.addBoth(lambda _: reactor.stop())\n",
    "    reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframe\n",
    "df = pd.DataFrame(columns = [\"Name\", \"Subscribers\", \"Genre\", \"Link\", \"Num videos\", \n",
    "                            \"Median views\", \"Projected Earnings\", \"Mean views\", \"Date Added\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
